{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d82bbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np                                                   #for numerical operations\n",
    "import pandas as pd                                                  #for manipulation\n",
    "import matplotlib.pyplot as plt                                      #for creating interactive visualizations\n",
    "import os\n",
    "import pickle                                                        #used for saving/loading trained machine learning models\n",
    "import tensorflow as tf                                              #for building/training deep learning models\n",
    "from tensorflow import keras                                         #provide interface for building/training neural networks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer            #to convert text into a sequence of tokens or words \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense           #Embedding->word embeddings,\n",
    "                                                                     #LSTM ->type of RNN layer,Dense->fully connected layer\n",
    "from tensorflow.keras.models import Sequential         #linear stack of layers in Keras(allow us to build model layer by layer)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  #ensure that all sequences in a list have the same length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4b2696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the corpus is: : 610921\n"
     ]
    }
   ],
   "source": [
    "#read the data file\n",
    "path=r\"C:\\Users\\taman\\Downloads\\Sherlock Holmes Dataset.txt\"         #path of your text file\n",
    "text = open(path).read().lower()                                     #read and convert it into lowercase\n",
    "print('length of the corpus is: :', len(text))                       #checking length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c0e8719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8200"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "#-----Tokenization------process of breaking down a text into smaller units called tokens\n",
    "#Create a tokenizer\n",
    "tokenizer = Tokenizer()                   \n",
    "#Fit the tokenizer on the text data\n",
    "tokenizer.fit_on_texts([text])   #pass text as input then analyze text,builds a vocabulary of unique words/assigns numerical index to each\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "total_words                      #total number of distinct words in the text     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69f5af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "#Loop through each line in the text\n",
    "for line in text.split('\\n'):                              #assuming 'text' is a multiline string then split text into lines\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]   #Tokenize the current line using the tokenizer\n",
    "    # Create n-gram sequences from the tokenized line\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ce09e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the maximum length among all the sequences \n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "\n",
    "#pad_sequences-->ensure all sequences in input_sequences have same length,\n",
    "#max_sequence->maximum length of the sequences after padding,\n",
    "#'pre'->padding should be added to the beginning of each sequence\n",
    "#np.array->convert list of sequences into numpy array\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e496467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input sequences are split into two arrays, ‘X’ and ‘y’\n",
    "X = input_sequences[:, :-1]  #except for the last column\n",
    "y = input_sequences[:, -1]   #values of the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72f302a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,    1],\n",
       "       [   0,    0,    0, ...,    0,    1, 1561],\n",
       "       [   0,    0,    0, ...,    1, 1561,    5],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   28,    1, 8198],\n",
       "       [   0,    0,    0, ...,    1, 8198, 8199],\n",
       "       [   0,    0,    0, ..., 8198, 8199, 3187]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "430e7b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1561,    5,  129, ..., 8199, 3187, 3186])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d9ee9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming a list of class labels y into a NumPy array\n",
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f3d102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 17, 100)           820000    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8200)              1057800   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,995,048\n",
      "Trainable params: 1,995,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Adding an Embedding layer\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "\n",
    "# Adding an LSTM layer\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# Adding a Dense layer\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Printing the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bdce75ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 5.7822 - accuracy: 0.1166\n",
      "Epoch 2/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 5.2596 - accuracy: 0.1427\n",
      "Epoch 3/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 4.8976 - accuracy: 0.1602\n",
      "Epoch 4/50\n",
      "3010/3010 [==============================] - 372s 124ms/step - loss: 4.5778 - accuracy: 0.1799\n",
      "Epoch 5/50\n",
      "3010/3010 [==============================] - 65s 22ms/step - loss: 4.2878 - accuracy: 0.2000\n",
      "Epoch 6/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 4.0182 - accuracy: 0.2246\n",
      "Epoch 7/50\n",
      "3010/3010 [==============================] - 127s 42ms/step - loss: 3.7651 - accuracy: 0.2526\n",
      "Epoch 8/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 3.5243 - accuracy: 0.2848\n",
      "Epoch 9/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 3.3005 - accuracy: 0.3187\n",
      "Epoch 10/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 3.0915 - accuracy: 0.3519\n",
      "Epoch 11/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 2.8988 - accuracy: 0.3846\n",
      "Epoch 12/50\n",
      "3010/3010 [==============================] - 68s 22ms/step - loss: 2.7205 - accuracy: 0.4175\n",
      "Epoch 13/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 2.5587 - accuracy: 0.4476\n",
      "Epoch 14/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 2.4068 - accuracy: 0.4751\n",
      "Epoch 15/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 2.2680 - accuracy: 0.5037\n",
      "Epoch 16/50\n",
      "3010/3010 [==============================] - 535s 178ms/step - loss: 2.1399 - accuracy: 0.5298\n",
      "Epoch 17/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 2.0239 - accuracy: 0.5526\n",
      "Epoch 18/50\n",
      "3010/3010 [==============================] - 79s 26ms/step - loss: 1.9159 - accuracy: 0.5754\n",
      "Epoch 19/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 1.8185 - accuracy: 0.5957\n",
      "Epoch 20/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 1.7266 - accuracy: 0.6154\n",
      "Epoch 21/50\n",
      "3010/3010 [==============================] - 68s 23ms/step - loss: 1.6452 - accuracy: 0.6316\n",
      "Epoch 22/50\n",
      "3010/3010 [==============================] - 68s 22ms/step - loss: 1.5676 - accuracy: 0.6496\n",
      "Epoch 23/50\n",
      "3010/3010 [==============================] - 68s 22ms/step - loss: 1.4959 - accuracy: 0.6649\n",
      "Epoch 24/50\n",
      "3010/3010 [==============================] - 207s 69ms/step - loss: 1.4311 - accuracy: 0.6785\n",
      "Epoch 25/50\n",
      "3010/3010 [==============================] - 68s 22ms/step - loss: 1.3687 - accuracy: 0.6928\n",
      "Epoch 26/50\n",
      "3010/3010 [==============================] - 68s 23ms/step - loss: 1.3150 - accuracy: 0.7062\n",
      "Epoch 27/50\n",
      "3010/3010 [==============================] - 70s 23ms/step - loss: 1.2615 - accuracy: 0.7179\n",
      "Epoch 28/50\n",
      "3010/3010 [==============================] - 68s 22ms/step - loss: 1.2141 - accuracy: 0.7267\n",
      "Epoch 29/50\n",
      "3010/3010 [==============================] - 68s 23ms/step - loss: 1.1717 - accuracy: 0.7358\n",
      "Epoch 30/50\n",
      "3010/3010 [==============================] - 68s 23ms/step - loss: 1.1291 - accuracy: 0.7442\n",
      "Epoch 31/50\n",
      "3010/3010 [==============================] - 68s 23ms/step - loss: 1.0942 - accuracy: 0.7516\n",
      "Epoch 32/50\n",
      "3010/3010 [==============================] - 68s 23ms/step - loss: 1.0568 - accuracy: 0.7595\n",
      "Epoch 33/50\n",
      "3010/3010 [==============================] - 69s 23ms/step - loss: 1.0271 - accuracy: 0.7668\n",
      "Epoch 34/50\n",
      "3010/3010 [==============================] - 69s 23ms/step - loss: 0.9956 - accuracy: 0.7734\n",
      "Epoch 35/50\n",
      "3010/3010 [==============================] - 68s 23ms/step - loss: 0.9697 - accuracy: 0.7777\n",
      "Epoch 36/50\n",
      "3010/3010 [==============================] - 68s 23ms/step - loss: 0.9431 - accuracy: 0.7843\n",
      "Epoch 37/50\n",
      "3010/3010 [==============================] - 69s 23ms/step - loss: 0.9201 - accuracy: 0.7891\n",
      "Epoch 38/50\n",
      "3010/3010 [==============================] - 69s 23ms/step - loss: 0.8953 - accuracy: 0.7941\n",
      "Epoch 39/50\n",
      "3010/3010 [==============================] - 68s 23ms/step - loss: 0.8763 - accuracy: 0.7970\n",
      "Epoch 40/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 0.8571 - accuracy: 0.8023\n",
      "Epoch 41/50\n",
      "3010/3010 [==============================] - 1015s 337ms/step - loss: 0.8386 - accuracy: 0.8055\n",
      "Epoch 42/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 0.8236 - accuracy: 0.8096\n",
      "Epoch 43/50\n",
      "3010/3010 [==============================] - 65s 22ms/step - loss: 0.8066 - accuracy: 0.8122\n",
      "Epoch 44/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 0.7906 - accuracy: 0.8150\n",
      "Epoch 45/50\n",
      "3010/3010 [==============================] - 65s 21ms/step - loss: 0.7799 - accuracy: 0.8181\n",
      "Epoch 46/50\n",
      "3010/3010 [==============================] - 65s 22ms/step - loss: 0.7653 - accuracy: 0.8203\n",
      "Epoch 47/50\n",
      "3010/3010 [==============================] - 65s 22ms/step - loss: 0.7537 - accuracy: 0.8226\n",
      "Epoch 48/50\n",
      "3010/3010 [==============================] - 65s 22ms/step - loss: 0.7449 - accuracy: 0.8250\n",
      "Epoch 49/50\n",
      "3010/3010 [==============================] - 65s 22ms/step - loss: 0.7336 - accuracy: 0.8249\n",
      "Epoch 50/50\n",
      "3010/3010 [==============================] - 65s 21ms/step - loss: 0.7243 - accuracy: 0.8286\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "#verbose->Controls amount of information printed during training i.e =1 (progress bars/information displayed for each epoch.)\n",
    "#epochs->number of times the model will iterate over the entire training dataset\n",
    "lstm=model.fit(X, y,epochs=50, verbose=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5be2b8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.7821946144104,\n",
       " 5.259610176086426,\n",
       " 4.8975725173950195,\n",
       " 4.5778350830078125,\n",
       " 4.287757873535156,\n",
       " 4.018226623535156,\n",
       " 3.7651207447052,\n",
       " 3.5243349075317383,\n",
       " 3.3005177974700928,\n",
       " 3.09148907661438,\n",
       " 2.8988306522369385,\n",
       " 2.720496654510498,\n",
       " 2.5586838722229004,\n",
       " 2.406815767288208,\n",
       " 2.2679531574249268,\n",
       " 2.1399126052856445,\n",
       " 2.0239107608795166,\n",
       " 1.9158556461334229,\n",
       " 1.8185195922851562,\n",
       " 1.7265570163726807,\n",
       " 1.6451746225357056,\n",
       " 1.5676207542419434,\n",
       " 1.4959113597869873,\n",
       " 1.4310874938964844,\n",
       " 1.3686628341674805,\n",
       " 1.3150478601455688,\n",
       " 1.2614696025848389,\n",
       " 1.2141187191009521,\n",
       " 1.171701192855835,\n",
       " 1.1290698051452637,\n",
       " 1.0942432880401611,\n",
       " 1.0568344593048096,\n",
       " 1.0271456241607666,\n",
       " 0.9955662488937378,\n",
       " 0.9696739912033081,\n",
       " 0.9431201219558716,\n",
       " 0.9200921058654785,\n",
       " 0.895290732383728,\n",
       " 0.8762571811676025,\n",
       " 0.8570692539215088,\n",
       " 0.8386108875274658,\n",
       " 0.8235945701599121,\n",
       " 0.806557297706604,\n",
       " 0.7906132340431213,\n",
       " 0.7799181938171387,\n",
       " 0.7652570605278015,\n",
       " 0.7537423372268677,\n",
       " 0.7448998093605042,\n",
       " 0.7335831522941589,\n",
       " 0.7242851257324219]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a257cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "i found in the morning it was an evil\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"i found in\"\n",
    "next_words = 6\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e81bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
